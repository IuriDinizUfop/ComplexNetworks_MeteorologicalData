{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.metrics as sk\n",
    "import scipy \n",
    "import igraph\n",
    "import ts2vg # Time series to visibility graphs\n",
    "import scipy.spatial.distance as distance\n",
    "import scipy\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Global feature array\n",
    "        1.1. Diameter, density, heterogeinity parameter, assortativity degree\n",
    "2. Local feature array\n",
    "        2.1. Average degrees, avg. betweenness, avg. clustering, avg. closeness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def threshold(adj,threshold):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        adj - Dicionário cuja chave é o índice dos pontos comparados e o valores é a resultante da medida de\n",
    "        distância/similaridade utilizada. \n",
    "        \n",
    "        threshold - limiar a ser aplicado.\n",
    "    Returns:\n",
    "    --------\n",
    "        Nova lista de adjacência com os valores abaixo do limiar definido.\n",
    "    \"\"\"\n",
    "    new_adj = {}\n",
    "    for i,j in adj:\n",
    "        if (adj[i,j] <= threshold):\n",
    "            new_adj[i,j] = adj[i,j]\n",
    "    return(new_adj)\n",
    "    \n",
    "def geographical_distance(adj,pos):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        adj - pares de pontos que correspondem a um ponto (série temporal) disposta no espaço geográfico da \n",
    "        bacia do rio Tamanduateí.\n",
    "        pos - índice (i,j) de cada ponto na matriz que representa o espaço geográfico.\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "        A distância euclidiana entre os pontos distribuídos na região analisada.\n",
    "    \"\"\"\n",
    "    geo_dist = {}\n",
    "    for i in adj:\n",
    "        a = pos[i[0]]\n",
    "        b = pos[i[1]]\n",
    "        dist = np.sqrt((a[0]-b[0])**2+(a[1]-b[1])**2)\n",
    "        geo_dist[i] = dist\n",
    "    return(geo_dist)\n",
    "\n",
    "def norm(data):\n",
    "    \"\"\"\n",
    "    Parameters:\n",
    "    -----------\n",
    "        Vetor numérico.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "        Valores da vetor data entre o intervalo de 0 e 1.\n",
    "    \n",
    "    \"\"\"\n",
    "    n = []\n",
    "    max_value = max(data)\n",
    "    min_value = min(data)\n",
    "    for i in range(len(data)):\n",
    "        n.append((data[i]-min_value)/(max_value - min_value))\n",
    "    return(n)\n",
    "\n",
    "class reading():\n",
    "    \n",
    "    def files():\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        --------\n",
    "            Dados meteorológicos relacionados à região que compreende a bacia do rio Tamanduateí:\n",
    "            4464 arquivos em formato de matriz com 37 linhas e 36 colunas contendo a previsão de precipitação\n",
    "            para determinada região. Os valores -99 delimitam a região válida para as previsões.\n",
    "            Formato do nome: ppi-CZ-1-201501-1-0-0.txt\n",
    "                                      y   mo d h m\n",
    "            y: ano (2015)\n",
    "            mo: mês (1)\n",
    "            d: dia (1 - 31)\n",
    "            h: hora (0 - 23)  \n",
    "            m: minuto (00 - 50, a cada 10 minutos)\n",
    "        \"\"\"\n",
    "        data = {}\n",
    "        for day in range(0,31):\n",
    "            for hour in range(0,24):\n",
    "                for minutes in range(0,6):\n",
    "                    data[(day*144)+(hour*6)+(minutes)] = np.genfromtxt(\"01/ppi-CZ-1-201501-\"+str(day+1)+\"-\"+str(hour)+\"-\"+str(minutes*10)+\".txt\")\n",
    "\n",
    "        return(data)\n",
    "\n",
    "    def dataValidation(data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Matrizes retornadas pela função reading.files()\n",
    "        Returns:\n",
    "        --------\n",
    "            Array unidimensional contendo os valores válidos (excluindo os valores -99)\n",
    "            e um array contendo a posição dos dados na matriz original.\n",
    "        \"\"\"\n",
    "        new_data = {}\n",
    "        new_idxs = {}\n",
    "        aux_data = np.empty((0,0))\n",
    "        idx = np.empty((0,2))\n",
    "        for keys in range(len(data)):\n",
    "            for rows in range(len(data[keys][:,0])):\n",
    "                for columns in range(len(data[keys][0,:])):\n",
    "                    if (data[keys][rows,columns] != -99):\n",
    "                        aux_data = np.append(aux_data,data[keys][rows,columns])\n",
    "                        idx = np.append(idx,[[rows,columns]],axis=0)\n",
    "            new_data[keys] = aux_data\n",
    "            new_idxs[keys] = idx\n",
    "            aux_data = np.empty((0,0))\n",
    "            idx = np.empty((0,2))\n",
    "        \n",
    "        return(new_data,new_idxs[0])\n",
    "\n",
    "    def timeSeriesGeneration(valid_data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Arrays unidimensionais contendo os valores estimados das previsões\n",
    "        Returns:\n",
    "        --------\n",
    "            Série temporal associada a cada ponto válido (diferente de -99).\n",
    "            A série temporal 0 será composta por todos os valores na posição 0 dos 4464 arrays, e assim\n",
    "            em diante. O resultado são 587 séries temporais de tamanho 4464.\n",
    "        \"\"\"\n",
    "        time_series = {}\n",
    "        new_data = np.empty((0,0))\n",
    "        for time in range(len(valid_data[0])):\n",
    "            for data in range(len(valid_data)):\n",
    "                new_data = np.append(new_data,valid_data[data][time])\n",
    "            time_series[time] = new_data\n",
    "            new_data = np.empty((0,0))\n",
    "        \n",
    "        return(time_series)\n",
    "\n",
    "class network():\n",
    "    \n",
    "    def geo_graph(adj):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        \"\"\"\n",
    "        geo_graph = igraph.Graph()\n",
    "        geo_graph.add_vertices(587) # Número de séries temporais\n",
    "        for i in range(geo_graph.vcount()):\n",
    "            geo_graph.vs[i][\"label\"] = i # Enumerando os nós de 0 a 586\n",
    "        geo_graph.add_edges(adj)\n",
    "\n",
    "    def graphs(data):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Lista de valores inteiros que correspondem aos nós de uma rede. Os valores devem ser conectados\n",
    "            em sequência.\n",
    "            Exemplo:\n",
    "                data = (1,2,8,4,5,2,7,9,4)\n",
    "                adjacency_list from data: (1,2),(2,8),(8,4),(4,5),(5,2),(2,7),(7,9),(9,4)\n",
    "            Assim, um grafo seria construído usando o conjunto data como os nós da rede e o conjunto\n",
    "            adjacency_list seria responsável pela conexão dos nós.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "            graph_metrics - Dicionário normalizado contendo as medidas average degree, average betweenness,\n",
    "            average closeness, density e diameter.\n",
    "            graph_degree - Array contendo o degree de cada nó da rede.\n",
    "            graph_betweenness - Array contendo o betweenness de cada nó da rede.\n",
    "            graph_closeness - Array contendo o closeness de cada nó da rede.\n",
    "        \"\"\"\n",
    "\n",
    "        # Graph creation from decimal integer array\n",
    "\n",
    "        graph = igraph.Graph()\n",
    "\n",
    "        #graph.add_vertices(max(data)+1)\n",
    "        graph.add_vertices(len(data))\n",
    "\n",
    "        #adjacency_list = np.empty((0,0), int)\n",
    "        aux_list = []\n",
    "        adjacency_list = []\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            if not(len(data)-1 == i):\n",
    "                aux_list.append([data[i],data[i+1]])\n",
    "\n",
    "        for i,j in aux_list:\n",
    "            if not (([i,j] in adjacency_list) or ([j,i] in adjacency_list) or (i == j)):\n",
    "                adjacency_list.append([i,j])\n",
    "\n",
    "        for i in range(len(data)):\n",
    "            graph.vs[i][\"label\"] = i\n",
    "\n",
    "        graph.add_edges(adjacency_list) # Full graph, with disconnected nodes\n",
    "\n",
    "        # Only nodes with degree > 0\n",
    "\n",
    "        disconnected = np.empty((0,0), int)\n",
    "        for i in range(graph.vcount()):\n",
    "            if (graph.vs[i].degree() == 0):\n",
    "                disconnected = np.append(disconnected, i)\n",
    "\n",
    "        graph.delete_vertices(disconnected)\n",
    "        \n",
    "        all_metrics, seq_metrics, global_metrics, local_metrics = network.metrics(graph)\n",
    "        \n",
    "        return(all_metrics, seq_metrics, global_metrics, local_metrics)\n",
    "        \n",
    "    def metrics(g):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Grafo criado na biblioteca igraph\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            graph_metrics - Dicionário normalizado contendo as medidas average degree, average betweenness,\n",
    "            average closeness, density e diameter.\n",
    "            graph_degree - Array contendo o degree de cada nó da rede.\n",
    "            graph_betweenness - Array contendo o betweenness de cada nó da rede.\n",
    "            graph_closeness - Array contendo o closeness de cada nó da rede.            \n",
    "        \"\"\"\n",
    "\n",
    "        def heterogeinity(degree):\n",
    "            power_degree = [] # degree sequence \n",
    "            for i in degree:\n",
    "                power_degree.append(i**2)\n",
    "                hp = ( (sum(power_degree)/len(power_degree))/((sum(degree)/len(degree))**2) )\n",
    "            return(hp)\n",
    "\n",
    "        # Degree and Average Degree\n",
    "        degree_sequence = [round(i,4) for i in g.degree()]\n",
    "        avg_degree = round((sum(degree_sequence)/len(degree_sequence)),4)\n",
    "\n",
    "        # Diameter\n",
    "        diameter = round(g.diameter(),4)\n",
    "\n",
    "        # Betweenness and Average Betweenness\n",
    "        betweenness_sequence = [round(i,4) for i in g.betweenness()]\n",
    "        avg_betweenness = round((sum(betweenness_sequence)/len(betweenness_sequence)),4)\n",
    "\n",
    "        # Closeness and Average Closeness\n",
    "        closeness_sequence = [round(i,4) for i in g.closeness()]\n",
    "        avg_closeness = round(sum(closeness_sequence)/len(closeness_sequence),4)\n",
    "\n",
    "        # Density\n",
    "        density = round(g.density(),4)\n",
    "        \n",
    "        # Assortativity Degree\n",
    "        assor_degree = round(g.assortativity_degree(),4)\n",
    "        \n",
    "        # Heterogeinity parameter\n",
    "        hp = round(heterogeinity(g.degree()),4)\n",
    "\n",
    "        feature_dict = {\n",
    "            'Avg_Betweenness': avg_betweenness,\n",
    "            'Avg_Closeness': avg_closeness,\n",
    "            'Avg_Degree': avg_degree,\n",
    "            'Diameter': diameter,\n",
    "            'Density': density\n",
    "        }\n",
    "        \n",
    "        seq_dict = {\n",
    "            'Betweenness': betweenness_sequence,\n",
    "            'Closeness': closeness_sequence,\n",
    "            'Degree': degree_sequence,\n",
    "        }\n",
    "        \n",
    "        feature_global_dict = {\n",
    "            'Assortativity_Degree': assor_degree,\n",
    "            'Heterogeinity_Parameter': hp,\n",
    "            'Density': density,\n",
    "            'Diameter': diameter\n",
    "        }\n",
    "    \n",
    "        feature_local_dict = {\n",
    "            'Avg_Betweenness': avg_betweenness,\n",
    "            'Avg_Closeness': avg_closeness,\n",
    "            'Avg_Degree': avg_degree\n",
    "        }\n",
    "        \n",
    "        return(feature_dict,seq_dict,feature_global_dict,feature_local_dict)\n",
    "\n",
    "class algorithms():\n",
    "    \n",
    "    def DCSD(time_series,n): # Dynamical Characterization with Symbolic Dynamics - DCSD\n",
    "        '''\n",
    "        Parameters:\n",
    "        -----------\n",
    "            time_series - Série temporal a ser convertida em grafo.\n",
    "            n - tamanho da palavra binária usada na conversão binário/decimal\n",
    "            \n",
    "            Exemplo de aplicação do algoritmo:\n",
    "                1. Definir o valor máximo e mínimo de uma série temporal:\n",
    "                    1.1. x = [5,8,9,12,10,7,4,3,5,7,11]\n",
    "                    1.2. Max: 12\n",
    "                         Min: 3\n",
    "                2. Definir o centro da série temporal:\n",
    "                    2.1. centro: (12+3)/2 = 7.5\n",
    "                3. Gerar um array binário (b), tal como se segue:\n",
    "                    3.1. x(i) >= centro, então b(i) = 1\n",
    "                    3.2. x(i) < centro, então b(i) = 0\n",
    "                    3.3. b = [0,1,1,1,1,0,0,0,0,0,1]\n",
    "                4. Conversão da série binária em uma série decimal:\n",
    "                    4.1. A conversão será feita através de palavras binárias de tamanho N.\n",
    "                    4.2. O grupamento de tamanho 4 terá um comportamento deslizante, sempre com um passo\n",
    "                    à direita:\n",
    "                        4.2.1. Assumindo N = 4:\n",
    "                                [|0,1,1,1|,1,0,0,0,0,0,1]\n",
    "                                     7\n",
    "                                [0,|1,1,1,1|,0,0,0,0,0,1]\n",
    "                                       15\n",
    "                                [0,1,|1,1,1,0|,0,0,0,0,1]\n",
    "                                         14\n",
    "                                [0,1,1,|1,1,0,0|,0,0,0,1]\n",
    "                                          12\n",
    "                                [0,1,1,1,|1,0,0,0|,0,0,1]\n",
    "                                             8\n",
    "                                [0,1,1,1,1,|0,0,0,0|,0,1]\n",
    "                                               0\n",
    "                                [0,1,1,1,1,0,|0,0,0,0|,1]\n",
    "                                                 0\n",
    "                                [0,1,1,1,1,0,0,|0,0,0,1|]\n",
    "                                                   1\n",
    "\n",
    "                                Nova série decimal inteira = [7,15,14,12,8,0,0,1]\n",
    "\n",
    "                5. Cada valor da nova série gerada corresponde a um nó e a conexão entre esses nós se dá\n",
    "                através da sequência dos valores, como se segue: (7,15), (15,14), (14,12), e assim por diante.\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "                graph_metrics - Dicionário normalizado contendo as medidas average degree, average betweenness,\n",
    "                average closeness, density e diameter.\n",
    "                graph_degree - Array contendo o degree de cada nó da rede.\n",
    "                graph_betweenness - Array contendo o betweenness de cada nó da rede.\n",
    "                graph_closeness - Array contendo o closeness de cada nó da rede.\n",
    "\n",
    "            References:\n",
    "            -----------\n",
    "                FREITAS, Vander LS; LACERDA, Juliana C.; MACAU, Elbert EN.\n",
    "                Complex Networks Approach for Dynamical Characterization of Nonlinear Systems.\n",
    "                International Journal of Bifurcation and Chaos, v. 29, n. 13, p. 1950188, 2019.\n",
    "            '''\n",
    "        center = (max(time_series)+min(time_series))/2\n",
    "\n",
    "        # Converting time_series to binary\n",
    "\n",
    "        binary_array = np.empty((0,0), int)\n",
    "\n",
    "        for i in time_series:\n",
    "            if i >= center:\n",
    "                binary_array = np.append(binary_array,1)\n",
    "            else:\n",
    "                binary_array = np.append(binary_array,0)\n",
    "\n",
    "        # Converting the binary array to decimal (integer)\n",
    "\n",
    "        decimal_array = np.empty((0,0), int)\n",
    "\n",
    "        for i in range(len(binary_array)-n+1): \n",
    "            word = binary_array[i:i+n]\n",
    "            string_word = ''\n",
    "            for j in range(len(word)): \n",
    "                string_word = string_word + str(word[j])\n",
    "            # Converting to decimal\n",
    "            decimal_number = int(string_word,2)\n",
    "            decimal_array = np.append(decimal_array,decimal_number)     \n",
    "\n",
    "        all_metrics_DCSD, seq_metrics_DCSD, global_metrics_DCSD, local_metrics_DCSD = network.graphs(decimal_array)\n",
    "\n",
    "        return(all_metrics_DCSD, seq_metrics_DCSD, global_metrics_DCSD, local_metrics_DCSD)\n",
    "\n",
    "    def DCTIF(time_series, n): # Dynamical Characterization using the Top Integral Function - DCTIF\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            time_series - Série temporal a ser convertida em grafo.\n",
    "            n - Tamanho do intervalo em que os dados serão mapeados.            \n",
    "            Exemplo:\n",
    "                1. A função DCTIF é definida como: Yk = [N · xk ] = min{i ∈ Z | N · xk ≤ i}\n",
    "                    1.1. Exemplo: x = [0.467, 1.0, 0.933, 0.8, 0.533, 0.0, 0.0, 0.067, 0.0]\n",
    "                    1.2. Usando N = 5\n",
    "                    1.3. Yk[0] = min{i ∈ Z | 5 · 0.467 ≤ i} = min{i ∈ Z | 2.335 ≤ i} = 3\n",
    "                    1.4. Yk = [3,5,5,4,3,1,1,1,1]\n",
    "                    1.5. Cada valor da nova série gerada corresponde a um nó e a conexão entre eles se dá\n",
    "                    através da sequência dos valores, como se segue: (3,5), (5,5), (5,4), e assim em diante.\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "            graph_metrics - Dicionário normalizado contendo as medidas average degree, average betweenness,\n",
    "            average closeness, density e diameter.\n",
    "            graph_degree - Array contendo o degree de cada nó da rede.\n",
    "            graph_betweenness - Array contendo o betweenness de cada nó da rede.\n",
    "            graph_closeness - Array contendo o closeness de cada nó da rede. \n",
    "            \n",
    "        References:\n",
    "        -----------\n",
    "            FREITAS, Vander LS; LACERDA, Juliana C.; MACAU, Elbert EN.\n",
    "            Complex Networks Approach for Dynamical Characterization of Nonlinear Systems.\n",
    "            International Journal of Bifurcation and Chaos, v. 29, n. 13, p. 1950188, 2019.\n",
    "        \"\"\"\n",
    "        # Normalizing the data\n",
    "        time_series_normalized = np.empty((0,0), float)\n",
    "        min_value = min(time_series)\n",
    "        max_value = max(time_series)\n",
    "        for i in time_series:\n",
    "            time_series_normalized = np.append(time_series_normalized, ((i-min_value)/(max_value-min_value)))    \n",
    "\n",
    "        nodes_array = np.empty((0,0),int)\n",
    "        for i in time_series_normalized:\n",
    "            if (i*n == 0):\n",
    "                nodes_array = np.append(nodes_array,int(1))\n",
    "            elif (round(i*n) - i*n >= 0.0):\n",
    "                nodes_array = np.append(nodes_array,int(round(i*n)))\n",
    "            else:\n",
    "                nodes_array = np.append(nodes_array,int(round(i*n) + 1.0))\n",
    "\n",
    "        all_metrics_DCTIF, seq_metrics_DCTIF, global_metrics_DCTIF, local_metrics_DCTIF = network.graphs(nodes_array)\n",
    "\n",
    "        return(all_metrics_DCTIF, seq_metrics_DCTIF, global_metrics_DCTIF, local_metrics_DCTIF)\n",
    "\n",
    "    def VG(time_series):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Uma série temporal que será transformada em um grafo do tipo natural visibility graph. Cada ponto\n",
    "            da série será convertido em um nó, sendo que a conexão entre os nós é definida de acordo com um \n",
    "            critério de visibilidade.\n",
    "            Exemplo:\n",
    "                Critério de visibilidade: yc < yb + (ya - yb)*((tb - tc)/(tb - ta))\n",
    "                    Dois pontos quaisquer em uma série temporal, (ya,ta) e (yb,tb), serão conectados no grafo,\n",
    "                    se, e somente se, um terceiro ponto, (yc,tc), colocado entre eles for capaz de satisfazer\n",
    "                    o critério de visibilidade.\n",
    "                y = [5,8,9,12,10,7,4,3,5,7,11]\n",
    "                t = [0,1,2,3,4,5,6,7,8,9,10] -> eixo temporal \n",
    "                Avaliando os pontos ya = 5 e yb = 8:\n",
    "                - Não há pontos intermediários, yc = 0\n",
    "                - Critério:\n",
    "                    0 < 8 + (5-8)*((1-0)/(1-0))\n",
    "                    0 < 8 + (-3)*1 \n",
    "                    0 < 5\n",
    "                    Logo, os pontos 5 e 8 serão conectados.\n",
    "                Avaliando os pontos ya = 9 e yb = 10:\n",
    "                - Ponto intermediário: yc = 12\n",
    "                - Critério:\n",
    "                    12 < 10 + (9 - 10)*((4-3)/(4-2))\n",
    "                    12 < 10 + (-1)*(1/2)\n",
    "                    12 < 10 - 1/2\n",
    "                    12 < 9.5\n",
    "                    Condição não satisfeita. Sendo assim, os pontos 9 e 10 não seriam conectados na rede.\n",
    "                \n",
    "        Returns:\n",
    "        --------\n",
    "            graph_metrics - Dicionário normalizado contendo as medidas average degree, average betweenness,\n",
    "            average closeness, density e diameter.\n",
    "            graph_degree - Array contendo o degree de cada nó da rede.\n",
    "            graph_betweenness - Array contendo o betweenness de cada nó da rede.\n",
    "            graph_closeness - Array contendo o closeness de cada nó da rede.\n",
    "\n",
    "        References:\n",
    "        -----------\n",
    "            LACASA, Lucas et al. \n",
    "            From time series to complex networks: The visibility graph.\n",
    "            Proceedings of the National Academy of Sciences, v. 105, n. 13, p. 4972-4975, 2008.\n",
    "        \"\"\"\n",
    "        vg_igraph = ts2vg.NaturalVisibilityGraph(time_series).as_igraph()\n",
    "        \n",
    "        all_metrics_VG, seq_metrics_VG, global_metrics_VG, local_metrics_VG = network.metrics(vg_igraph)\n",
    "        \n",
    "        return(all_metrics_VG, seq_metrics_VG, global_metrics_VG, local_metrics_VG)\n",
    "\n",
    "class saving():\n",
    "    \n",
    "    def saving_metrics(all_metrics_DCSD,all_metrics_DCTIF,all_metrics_VG):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Recebe dicionários contendo métricas de todos os grafos para cada algoritmo. Cada dicionário contém \n",
    "            chaves que vão de 0 até 586, representando os 587 grafos. Os valores de cada chaves são dicionários\n",
    "            contendo as métricas average degree, average betweenness, average closeness, density e diameter.\n",
    "            Os dicionários são convertidos em DataFrames Pandas e depois são salvos no formato .csv.\n",
    "        \"\"\"\n",
    "        dcsd_df = pd.DataFrame(data=all_metrics_DCSD)\n",
    "        dctif_df = pd.DataFrame(data=all_metrics_DCTIF)\n",
    "        vg_df = pd.DataFrame(data=all_metrics_VG)\n",
    "\n",
    "        dcsd_df.T.to_csv('DCSD/DCSD-metrics.csv',index=False)\n",
    "        dctif_df.T.to_csv('DCTIF/DCTIF-metrics.csv',index=False)\n",
    "        vg_df.T.to_csv('VG/VG-metrics.csv',index=False)\n",
    "   \n",
    "    \n",
    "    def saving_sequences(seq_metrics_DCSD,seq_metrics_DCTIF,seq_metrics_VG):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        def check_length(data):\n",
    "            \"\"\"\n",
    "            Parameters:\n",
    "            -----------\n",
    "            \n",
    "            Returns:\n",
    "            --------\n",
    "            \"\"\"\n",
    "            if (len(data) != 4464):\n",
    "                for i in range(4464-len(data)):\n",
    "                    data.append(0)\n",
    "            return(data)\n",
    "\n",
    "        btw_DCSD = {}\n",
    "        clo_DCSD = {}\n",
    "        deg_DCSD = {}\n",
    "        btw_DCTIF = {}\n",
    "        clo_DCTIF = {}\n",
    "        deg_DCTIF = {}\n",
    "        btw_VG = {}\n",
    "        clo_VG = {}\n",
    "        deg_VG = {}\n",
    "        for i in range(len(seq_metrics_DCSD)):\n",
    "            btw_DCSD[i] = check_length(seq_metrics_DCSD[i]['Betweenness'])\n",
    "            clo_DCSD[i] = check_length(seq_metrics_DCSD[i]['Closeness'])\n",
    "            deg_DCSD[i] = check_length(seq_metrics_DCSD[i]['Degree'])\n",
    "            btw_DCTIF[i] = check_length(seq_metrics_DCTIF[i]['Betweenness'])\n",
    "            clo_DCTIF[i] = check_length(seq_metrics_DCTIF[i]['Closeness'])\n",
    "            deg_DCTIF[i] = check_length(seq_metrics_DCTIF[i]['Degree'])\n",
    "            btw_VG[i] = check_length(seq_metrics_VG[i]['Betweenness'])\n",
    "            clo_VG[i] = check_length(seq_metrics_VG[i]['Closeness'])\n",
    "            deg_VG[i] = check_length(seq_metrics_VG[i]['Degree'])\n",
    "\n",
    "        #DCSD\n",
    "        btw_DCSD_df = pd.DataFrame(data=btw_DCSD)\n",
    "        clo_DCSD_df = pd.DataFrame(data=clo_DCSD)\n",
    "        deg_DCSD_df = pd.DataFrame(data=deg_DCSD)\n",
    "\n",
    "        btw_DCSD_df.to_csv('DCSD/DCSD-betweenness.csv',index=False)\n",
    "        clo_DCSD_df.to_csv('DCSD/DCSD-closeness.csv',index=False)\n",
    "        deg_DCSD_df.to_csv('DCSD/DCSD-degree.csv',index=False)\n",
    "\n",
    "        #DCTIF\n",
    "        btw_DCTIF_df = pd.DataFrame(data=btw_DCTIF)\n",
    "        clo_DCTIF_df = pd.DataFrame(data=clo_DCTIF)\n",
    "        deg_DCTIF_df = pd.DataFrame(data=deg_DCTIF)\n",
    "\n",
    "        btw_DCTIF_df.to_csv('DCTIF/DCTIF-betweenness.csv',index=False)\n",
    "        clo_DCTIF_df.to_csv('DCTIF/DCTIF-closeness.csv',index=False)\n",
    "        deg_DCTIF_df.to_csv('DCTIF/DCTIF-degree.csv',index=False)    \n",
    "\n",
    "        #VG\n",
    "        btw_VG_df = pd.DataFrame(data=btw_VG)\n",
    "        clo_VG_df = pd.DataFrame(data=clo_VG)\n",
    "        deg_VG_df = pd.DataFrame(data=deg_VG)\n",
    "\n",
    "        btw_VG_df.to_csv('VG/VG-betweenness.csv',index=False)\n",
    "        clo_VG_df.to_csv('VG/VG-closeness.csv',index=False)\n",
    "        deg_VG_df.to_csv('VG/VG-degree.csv',index=False) \n",
    "    \n",
    "    def saving_global_metrics(global_metrics_DCSD,global_metrics_DCTIF,global_metrics_VG):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        global_metrics_DCSD_df = pd.DataFrame(data=global_metrics_DCSD)\n",
    "        global_metrics_DCTIF_df = pd.DataFrame(data=global_metrics_DCTIF)\n",
    "        global_metrics_VG_df = pd.DataFrame(data=global_metrics_VG)\n",
    "\n",
    "        global_metrics_DCSD_df.T.to_csv('DCSD/DCSD-global-metrics.csv',index=False)\n",
    "        global_metrics_DCTIF_df.T.to_csv('DCTIF/DCTIF-global-metrics.csv',index=False)\n",
    "        global_metrics_VG_df.T.to_csv('VG/VG-global-metrics.csv',index=False)\n",
    "\n",
    "    def saving_local_metrics(local_metrics_DCSD,local_metrics_DCTIF,local_metrics_VG):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "        \"\"\"\n",
    "        local_metrics_DCSD_df = pd.DataFrame(data=local_metrics_DCSD)\n",
    "        local_metrics_DCTIF_df = pd.DataFrame(data=local_metrics_DCTIF)\n",
    "        local_metrics_VG_df = pd.DataFrame(data=local_metrics_VG)\n",
    "\n",
    "        local_metrics_DCSD_df.T.to_csv('DCSD/DCSD-local-metrics.csv',index=False)\n",
    "        local_metrics_DCTIF_df.T.to_csv('DCTIF/DCTIF-local-metrics.csv',index=False)\n",
    "        local_metrics_VG_df.T.to_csv('VG/VG-local-metrics.csv',index=False)\n",
    "    \n",
    "        \n",
    "class comparing():\n",
    "    \n",
    "    def __init__(self, graph_metrics):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        -----------\n",
    "            Array de métricas que será usado como parâmetro para o restante das funções dessa classe.\n",
    "            São recebidos tanto os feature arrays, com métricas combinadas, quanto os sequence arrays, como é\n",
    "            o caso da sequência de graus.\n",
    "            As métricas enviadas são as pertencentes a todos os grafos. Sendo assim, self.metrics_array\n",
    "            receberá um array de tamanho 587, onde cada posição terá um array de métricas correspondente a\n",
    "            um grafo.\n",
    "            As funções de distância/similaridade irão computar a medida entre todos os pares (a,b) de métricas,\n",
    "            com a e b variando de 0 até 586, apenas desconsiderando casos como: \n",
    "                - a = b\n",
    "                - (b,a) se já existir um valor calculado para o par (a,b)\n",
    "        \"\"\"\n",
    "        self.metrics_array = graph_metrics\n",
    "    \n",
    "    def jensenshannon(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        --------\n",
    "            Retorna a o valor da distância de Jensen-Shannon, no formato de um dicionário, entre todos os \n",
    "            pares de grafos.\n",
    "            A função computa a raiz quadrada da divergência de Jensen-Shannon:\n",
    "                JSD(P||Q) = ((D(P||M) + D(Q||M))/2)^(1/2)\n",
    "                Onde:\n",
    "                    D é a divergência de Kullback-Leibler \n",
    "                    M = (P + Q)/2 \n",
    "        \n",
    "        Reference:\n",
    "        ----------\n",
    "            SciPy community. SciPy Reference Guide. <https://docs.scipy.org/doc/scipy/scipy-ref-1.5.4.pdf>, \n",
    "            v. 1.5.4, p. 2205.\n",
    "                \n",
    "        \"\"\"\n",
    "        jensenshannon_dist = {}\n",
    "        ctrl = 1\n",
    "        for i in range(len(self.metrics_array)):\n",
    "            for j in range(ctrl,len(self.metrics_array)):\n",
    "                jensenshannon_dist[i,j] = distance.jensenshannon(self.metrics_array[i],self.metrics_array[j])\n",
    "            ctrl += 1\n",
    "        return(jensenshannon_dist)    \n",
    "    \n",
    "    \n",
    "    def euclidean(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        --------\n",
    "            Retorna a o valor da distância Euclidiana, no formato de um dicionário, entre todos os \n",
    "            pares de grafos.\n",
    "            Considerando os vetores, a e b, a distância euclidiana é calculada como:\n",
    "                e_dist(a,b) = ||a - b||\n",
    "        \n",
    "        Reference:\n",
    "        ----------\n",
    "            SciPy community. SciPy Reference Guide. <https://docs.scipy.org/doc/scipy/scipy-ref-1.5.4.pdf>, \n",
    "            v. 1.5.4, p. 2204-2205.\n",
    "            \n",
    "        \"\"\"\n",
    "        euclidean_dist = {}\n",
    "        ctrl = 1\n",
    "        for i in range(len(self.metrics_array)):\n",
    "            for j in range(ctrl,len(self.metrics_array)):\n",
    "                euclidean_dist[i,j] = distance.euclidean(self.metrics_array[i],self.metrics_array[j])\n",
    "            ctrl += 1\n",
    "        return(euclidean_dist)\n",
    "\n",
    "    def manhattan(self):\n",
    "        \"\"\"    \n",
    "        Returns:\n",
    "        --------\n",
    "            Retorna a o valor da distância de Manhattan (Cityblock), no formato de um dicionário, entre \n",
    "            todos os pares de grafos.\n",
    "            Considerando os vetores a e b, a distância de manhattan é calculada como:\n",
    "                m_dist(a,b) = Σ(|ai - bi|)\n",
    "                i deve ser iterado a fim de percorrer todos os elementos de cada vetor.\n",
    "\n",
    "        Reference:\n",
    "        ----------\n",
    "            SciPy community. SciPy Reference Guide. <https://docs.scipy.org/doc/scipy/scipy-ref-1.5.4.pdf>, \n",
    "            v. 1.5.4, p. 2203.\n",
    "        \"\"\"\n",
    "        manhattan_dist = {}\n",
    "        ctrl = 1\n",
    "        for i in range(len(self.metrics_array)):\n",
    "            for j in range(ctrl,len(self.metrics_array)):\n",
    "                manhattan_dist[i,j] = distance.cityblock(self.metrics_array[i],self.metrics_array[j])\n",
    "            ctrl += 1\n",
    "        return(manhattan_dist)\n",
    "\n",
    "    def canberra(self):\n",
    "        \"\"\"   \n",
    "        Returns:\n",
    "        --------\n",
    "            Retorna a o valor da distância de Canberra, no formato de um dicionário, entre \n",
    "            todos os pares de grafos.\n",
    "            Considerando os vetores a e b, a distância de canberra é calculada como:\n",
    "                c_dist(a,b) = Σ((|ai - bi|)/(|ai|+|bi|))\n",
    "                i deve ser iterado a fim de percorrer todos os elementos de cada vetor\n",
    "\n",
    "        Reference:\n",
    "        ----------\n",
    "            SciPy community. SciPy Reference Guide. <https://docs.scipy.org/doc/scipy/scipy-ref-1.5.4.pdf>, \n",
    "            v. 1.5.4, p. 2202.\n",
    "        \"\"\"\n",
    "        canberra_dist = {}\n",
    "        ctrl = 1\n",
    "        for i in range(len(self.metrics_array)):\n",
    "            for j in range(ctrl,len(self.metrics_array)):\n",
    "                canberra_dist[i,j] = distance.canberra(self.metrics_array[i],self.metrics_array[j])\n",
    "            ctrl += 1\n",
    "        return(canberra_dist)\n",
    "\n",
    "    def cosine(self):\n",
    "        \"\"\"    \n",
    "        Returns:\n",
    "        --------\n",
    "            Retorna a o valor da distância baseada em cosseno, no formato de um dicionário, entre \n",
    "            todos os pares de grafos.\n",
    "            Considerando os vetores a e b, o cosseno entre esses dois vetores pode ser obtido pela\n",
    "            seguinte relação:\n",
    "                cos(θ) = (a·b)/(||a||*||b||)\n",
    "                cos_dist = 1 - cos(θ)\n",
    "\n",
    "                (a·b) é o produto escalar entre os dois vetores.\n",
    "                \n",
    "        Reference:\n",
    "        ----------\n",
    "            SciPy community. SciPy Reference Guide. <https://docs.scipy.org/doc/scipy/scipy-ref-1.5.4.pdf>, \n",
    "            v. 1.5.4, p. 2204.\n",
    "        \"\"\"\n",
    "        cosine_sim = {}\n",
    "        ctrl = 1\n",
    "        for i in range(len(self.metrics_array)):\n",
    "            for j in range(ctrl,len(self.metrics_array)):\n",
    "                cosine_sim[i,j] = distance.cosine(self.metrics_array[i],self.metrics_array[j])\n",
    "            ctrl += 1\n",
    "        return(cosine_sim)\n",
    "# -------------------------------------------------------------------------------------------------------------    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading files and time series generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.99223184585571\n"
     ]
    }
   ],
   "source": [
    "# Estimated execution time: 59,17s\n",
    "init = time.time()\n",
    "\n",
    "# Time series\n",
    "files = reading.files()\n",
    "validated_files,position = reading.dataValidation(files) # removing -99 values\n",
    "meteorological_time_series = reading.timeSeriesGeneration(validated_files)\n",
    "\n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 de 586\n",
      "1 de 586\n",
      "2 de 586\n",
      "3 de 586\n",
      "4 de 586\n",
      "5 de 586\n",
      "6 de 586\n",
      "7 de 586\n",
      "8 de 586\n",
      "9 de 586\n",
      "10 de 586\n",
      "11 de 586\n",
      "12 de 586\n",
      "13 de 586\n",
      "14 de 586\n",
      "15 de 586\n",
      "16 de 586\n",
      "17 de 586\n",
      "18 de 586\n",
      "19 de 586\n",
      "20 de 586\n",
      "21 de 586\n",
      "22 de 586\n",
      "23 de 586\n",
      "24 de 586\n",
      "25 de 586\n",
      "26 de 586\n",
      "27 de 586\n",
      "28 de 586\n",
      "29 de 586\n",
      "30 de 586\n",
      "31 de 586\n",
      "32 de 586\n",
      "33 de 586\n",
      "34 de 586\n",
      "35 de 586\n",
      "36 de 586\n",
      "37 de 586\n",
      "38 de 586\n",
      "39 de 586\n",
      "40 de 586\n",
      "41 de 586\n",
      "42 de 586\n",
      "43 de 586\n",
      "44 de 586\n",
      "45 de 586\n",
      "46 de 586\n",
      "47 de 586\n",
      "48 de 586\n",
      "49 de 586\n",
      "50 de 586\n",
      "51 de 586\n",
      "52 de 586\n",
      "53 de 586\n",
      "54 de 586\n",
      "55 de 586\n",
      "56 de 586\n",
      "57 de 586\n",
      "58 de 586\n",
      "59 de 586\n",
      "60 de 586\n",
      "61 de 586\n",
      "62 de 586\n",
      "63 de 586\n",
      "64 de 586\n",
      "65 de 586\n",
      "66 de 586\n",
      "67 de 586\n",
      "68 de 586\n",
      "69 de 586\n",
      "70 de 586\n",
      "71 de 586\n",
      "72 de 586\n",
      "73 de 586\n",
      "74 de 586\n",
      "75 de 586\n",
      "76 de 586\n",
      "77 de 586\n",
      "78 de 586\n",
      "79 de 586\n",
      "80 de 586\n",
      "81 de 586\n",
      "82 de 586\n",
      "83 de 586\n",
      "84 de 586\n",
      "85 de 586\n",
      "86 de 586\n",
      "87 de 586\n",
      "88 de 586\n",
      "89 de 586\n",
      "90 de 586\n",
      "91 de 586\n",
      "92 de 586\n",
      "93 de 586\n",
      "94 de 586\n",
      "95 de 586\n",
      "96 de 586\n",
      "97 de 586\n",
      "98 de 586\n",
      "99 de 586\n",
      "100 de 586\n",
      "101 de 586\n",
      "102 de 586\n",
      "103 de 586\n",
      "104 de 586\n",
      "105 de 586\n",
      "106 de 586\n",
      "107 de 586\n",
      "108 de 586\n",
      "109 de 586\n",
      "110 de 586\n",
      "111 de 586\n",
      "112 de 586\n",
      "113 de 586\n",
      "114 de 586\n",
      "115 de 586\n",
      "116 de 586\n",
      "117 de 586\n",
      "118 de 586\n",
      "119 de 586\n",
      "120 de 586\n",
      "121 de 586\n",
      "122 de 586\n",
      "123 de 586\n",
      "124 de 586\n",
      "125 de 586\n",
      "126 de 586\n",
      "127 de 586\n",
      "128 de 586\n",
      "129 de 586\n",
      "130 de 586\n",
      "131 de 586\n",
      "132 de 586\n",
      "133 de 586\n",
      "134 de 586\n",
      "135 de 586\n",
      "136 de 586\n",
      "137 de 586\n",
      "138 de 586\n",
      "139 de 586\n",
      "140 de 586\n",
      "141 de 586\n",
      "142 de 586\n",
      "143 de 586\n",
      "144 de 586\n",
      "145 de 586\n",
      "146 de 586\n",
      "147 de 586\n",
      "148 de 586\n",
      "149 de 586\n",
      "150 de 586\n",
      "151 de 586\n",
      "152 de 586\n",
      "153 de 586\n",
      "154 de 586\n",
      "155 de 586\n",
      "156 de 586\n",
      "157 de 586\n",
      "158 de 586\n",
      "159 de 586\n",
      "160 de 586\n",
      "161 de 586\n",
      "162 de 586\n",
      "163 de 586\n",
      "164 de 586\n",
      "165 de 586\n",
      "166 de 586\n",
      "167 de 586\n",
      "168 de 586\n",
      "169 de 586\n",
      "170 de 586\n",
      "171 de 586\n",
      "172 de 586\n",
      "173 de 586\n",
      "174 de 586\n",
      "175 de 586\n",
      "176 de 586\n",
      "177 de 586\n",
      "178 de 586\n",
      "179 de 586\n",
      "180 de 586\n",
      "181 de 586\n",
      "182 de 586\n",
      "183 de 586\n",
      "184 de 586\n",
      "185 de 586\n",
      "186 de 586\n",
      "187 de 586\n",
      "188 de 586\n",
      "189 de 586\n",
      "190 de 586\n",
      "191 de 586\n",
      "192 de 586\n",
      "193 de 586\n",
      "194 de 586\n",
      "195 de 586\n",
      "196 de 586\n",
      "197 de 586\n",
      "198 de 586\n",
      "199 de 586\n",
      "200 de 586\n",
      "201 de 586\n",
      "202 de 586\n",
      "203 de 586\n",
      "204 de 586\n",
      "205 de 586\n",
      "206 de 586\n",
      "207 de 586\n",
      "208 de 586\n",
      "209 de 586\n",
      "210 de 586\n",
      "211 de 586\n",
      "212 de 586\n",
      "213 de 586\n",
      "214 de 586\n",
      "215 de 586\n",
      "216 de 586\n",
      "217 de 586\n",
      "218 de 586\n",
      "219 de 586\n",
      "220 de 586\n",
      "221 de 586\n",
      "222 de 586\n",
      "223 de 586\n",
      "224 de 586\n",
      "225 de 586\n",
      "226 de 586\n",
      "227 de 586\n",
      "228 de 586\n",
      "229 de 586\n",
      "230 de 586\n",
      "231 de 586\n",
      "232 de 586\n",
      "233 de 586\n",
      "234 de 586\n",
      "235 de 586\n",
      "236 de 586\n",
      "237 de 586\n",
      "238 de 586\n",
      "239 de 586\n",
      "240 de 586\n",
      "241 de 586\n",
      "242 de 586\n",
      "243 de 586\n",
      "244 de 586\n",
      "245 de 586\n",
      "246 de 586\n",
      "247 de 586\n",
      "248 de 586\n",
      "249 de 586\n",
      "250 de 586\n",
      "251 de 586\n",
      "252 de 586\n",
      "253 de 586\n",
      "254 de 586\n",
      "255 de 586\n",
      "256 de 586\n",
      "257 de 586\n",
      "258 de 586\n",
      "259 de 586\n",
      "260 de 586\n",
      "261 de 586\n",
      "262 de 586\n",
      "263 de 586\n",
      "264 de 586\n",
      "265 de 586\n",
      "266 de 586\n",
      "267 de 586\n",
      "268 de 586\n",
      "269 de 586\n",
      "270 de 586\n",
      "271 de 586\n",
      "272 de 586\n",
      "273 de 586\n",
      "274 de 586\n",
      "275 de 586\n",
      "276 de 586\n",
      "277 de 586\n",
      "278 de 586\n",
      "279 de 586\n",
      "280 de 586\n",
      "281 de 586\n",
      "282 de 586\n",
      "283 de 586\n",
      "284 de 586\n",
      "285 de 586\n",
      "286 de 586\n",
      "287 de 586\n",
      "288 de 586\n",
      "289 de 586\n",
      "290 de 586\n",
      "291 de 586\n",
      "292 de 586\n",
      "293 de 586\n",
      "294 de 586\n",
      "295 de 586\n",
      "296 de 586\n",
      "297 de 586\n",
      "298 de 586\n",
      "299 de 586\n",
      "300 de 586\n",
      "301 de 586\n",
      "302 de 586\n",
      "303 de 586\n",
      "304 de 586\n",
      "305 de 586\n",
      "306 de 586\n",
      "307 de 586\n",
      "308 de 586\n",
      "309 de 586\n",
      "310 de 586\n",
      "311 de 586\n",
      "312 de 586\n",
      "313 de 586\n",
      "314 de 586\n",
      "315 de 586\n",
      "316 de 586\n",
      "317 de 586\n",
      "318 de 586\n",
      "319 de 586\n",
      "320 de 586\n",
      "321 de 586\n",
      "322 de 586\n",
      "323 de 586\n",
      "324 de 586\n",
      "325 de 586\n",
      "326 de 586\n",
      "327 de 586\n",
      "328 de 586\n",
      "329 de 586\n",
      "330 de 586\n",
      "331 de 586\n",
      "332 de 586\n",
      "333 de 586\n",
      "334 de 586\n",
      "335 de 586\n",
      "336 de 586\n",
      "337 de 586\n",
      "338 de 586\n",
      "339 de 586\n",
      "340 de 586\n",
      "341 de 586\n",
      "342 de 586\n",
      "343 de 586\n",
      "344 de 586\n",
      "345 de 586\n",
      "346 de 586\n",
      "347 de 586\n",
      "348 de 586\n",
      "349 de 586\n",
      "350 de 586\n",
      "351 de 586\n",
      "352 de 586\n",
      "353 de 586\n",
      "354 de 586\n",
      "355 de 586\n",
      "356 de 586\n",
      "357 de 586\n",
      "358 de 586\n",
      "359 de 586\n",
      "360 de 586\n",
      "361 de 586\n",
      "362 de 586\n",
      "363 de 586\n",
      "364 de 586\n",
      "365 de 586\n",
      "366 de 586\n",
      "367 de 586\n",
      "368 de 586\n",
      "369 de 586\n",
      "370 de 586\n",
      "371 de 586\n",
      "372 de 586\n",
      "373 de 586\n",
      "374 de 586\n",
      "375 de 586\n",
      "376 de 586\n",
      "377 de 586\n",
      "378 de 586\n",
      "379 de 586\n",
      "380 de 586\n",
      "381 de 586\n",
      "382 de 586\n",
      "383 de 586\n",
      "384 de 586\n",
      "385 de 586\n",
      "386 de 586\n",
      "387 de 586\n",
      "388 de 586\n",
      "389 de 586\n",
      "390 de 586\n",
      "391 de 586\n",
      "392 de 586\n",
      "393 de 586\n",
      "394 de 586\n",
      "395 de 586\n",
      "396 de 586\n",
      "397 de 586\n",
      "398 de 586\n",
      "399 de 586\n",
      "400 de 586\n",
      "401 de 586\n",
      "402 de 586\n",
      "403 de 586\n",
      "404 de 586\n",
      "405 de 586\n",
      "406 de 586\n",
      "407 de 586\n",
      "408 de 586\n",
      "409 de 586\n",
      "410 de 586\n",
      "411 de 586\n",
      "412 de 586\n",
      "413 de 586\n",
      "414 de 586\n",
      "415 de 586\n",
      "416 de 586\n",
      "417 de 586\n",
      "418 de 586\n",
      "419 de 586\n",
      "420 de 586\n",
      "421 de 586\n",
      "422 de 586\n",
      "423 de 586\n",
      "424 de 586\n",
      "425 de 586\n",
      "426 de 586\n",
      "427 de 586\n",
      "428 de 586\n",
      "429 de 586\n",
      "430 de 586\n",
      "431 de 586\n",
      "432 de 586\n",
      "433 de 586\n",
      "434 de 586\n",
      "435 de 586\n",
      "436 de 586\n",
      "437 de 586\n",
      "438 de 586\n",
      "439 de 586\n",
      "440 de 586\n",
      "441 de 586\n",
      "442 de 586\n",
      "443 de 586\n",
      "444 de 586\n",
      "445 de 586\n",
      "446 de 586\n",
      "447 de 586\n",
      "448 de 586\n",
      "449 de 586\n",
      "450 de 586\n",
      "451 de 586\n",
      "452 de 586\n",
      "453 de 586\n",
      "454 de 586\n",
      "455 de 586\n",
      "456 de 586\n",
      "457 de 586\n",
      "458 de 586\n",
      "459 de 586\n",
      "460 de 586\n",
      "461 de 586\n",
      "462 de 586\n",
      "463 de 586\n",
      "464 de 586\n",
      "465 de 586\n",
      "466 de 586\n",
      "467 de 586\n",
      "468 de 586\n",
      "469 de 586\n",
      "470 de 586\n",
      "471 de 586\n",
      "472 de 586\n",
      "473 de 586\n",
      "474 de 586\n",
      "475 de 586\n",
      "476 de 586\n",
      "477 de 586\n",
      "478 de 586\n",
      "479 de 586\n",
      "480 de 586\n",
      "481 de 586\n",
      "482 de 586\n",
      "483 de 586\n",
      "484 de 586\n",
      "485 de 586\n",
      "486 de 586\n",
      "487 de 586\n",
      "488 de 586\n",
      "489 de 586\n",
      "490 de 586\n",
      "491 de 586\n",
      "492 de 586\n",
      "493 de 586\n",
      "494 de 586\n",
      "495 de 586\n",
      "496 de 586\n",
      "497 de 586\n",
      "498 de 586\n",
      "499 de 586\n",
      "500 de 586\n",
      "501 de 586\n",
      "502 de 586\n",
      "503 de 586\n",
      "504 de 586\n",
      "505 de 586\n",
      "506 de 586\n",
      "507 de 586\n",
      "508 de 586\n",
      "509 de 586\n",
      "510 de 586\n",
      "511 de 586\n",
      "512 de 586\n",
      "513 de 586\n",
      "514 de 586\n",
      "515 de 586\n",
      "516 de 586\n",
      "517 de 586\n",
      "518 de 586\n",
      "519 de 586\n",
      "520 de 586\n",
      "521 de 586\n",
      "522 de 586\n",
      "523 de 586\n",
      "524 de 586\n",
      "525 de 586\n",
      "526 de 586\n",
      "527 de 586\n",
      "528 de 586\n",
      "529 de 586\n",
      "530 de 586\n",
      "531 de 586\n",
      "532 de 586\n",
      "533 de 586\n",
      "534 de 586\n",
      "535 de 586\n",
      "536 de 586\n",
      "537 de 586\n",
      "538 de 586\n",
      "539 de 586\n",
      "540 de 586\n",
      "541 de 586\n",
      "542 de 586\n",
      "543 de 586\n",
      "544 de 586\n",
      "545 de 586\n",
      "546 de 586\n",
      "547 de 586\n",
      "548 de 586\n",
      "549 de 586\n",
      "550 de 586\n",
      "551 de 586\n",
      "552 de 586\n",
      "553 de 586\n",
      "554 de 586\n",
      "555 de 586\n",
      "556 de 586\n",
      "557 de 586\n",
      "558 de 586\n",
      "559 de 586\n",
      "560 de 586\n",
      "561 de 586\n",
      "562 de 586\n",
      "563 de 586\n",
      "564 de 586\n",
      "565 de 586\n",
      "566 de 586\n",
      "567 de 586\n",
      "568 de 586\n",
      "569 de 586\n",
      "570 de 586\n",
      "571 de 586\n",
      "572 de 586\n",
      "573 de 586\n",
      "574 de 586\n",
      "575 de 586\n",
      "576 de 586\n",
      "577 de 586\n",
      "578 de 586\n",
      "579 de 586\n",
      "580 de 586\n",
      "581 de 586\n",
      "582 de 586\n",
      "583 de 586\n",
      "584 de 586\n",
      "585 de 586\n",
      "586 de 586\n",
      "1782.104523897171\n"
     ]
    }
   ],
   "source": [
    "# Estimated execution time: 1783s\n",
    "init = time.time()\n",
    "\n",
    "all_metrics_VG = {}\n",
    "seq_metrics_VG = {}\n",
    "global_metrics_VG = {}\n",
    "local_metrics_VG = {}\n",
    "\n",
    "all_metrics_DCSD = {}\n",
    "seq_metrics_DCSD = {}\n",
    "global_metrics_DCSD = {}\n",
    "local_metrics_DCSD = {}\n",
    "\n",
    "all_metrics_DCTIF = {}\n",
    "seq_metrics_DCTIF = {}\n",
    "global_metrics_DCTIF = {}\n",
    "local_metrics_DCTIF = {}\n",
    "\n",
    "for idx in range(len(meteorological_time_series)):\n",
    "    print(idx,\"de\",(len(meteorological_time_series)-1))\n",
    "    all_metrics_VG[idx], seq_metrics_VG[idx], global_metrics_VG[idx], local_metrics_VG[idx] = algorithms.VG(meteorological_time_series[idx])\n",
    "    all_metrics_DCSD[idx], seq_metrics_DCSD[idx], global_metrics_DCSD[idx], local_metrics_DCSD[idx] = algorithms.DCSD(meteorological_time_series[idx],10)\n",
    "    all_metrics_DCTIF[idx], seq_metrics_DCTIF[idx], global_metrics_DCTIF[idx], local_metrics_DCTIF[idx] = algorithms.DCTIF(meteorological_time_series[idx],50)\n",
    "    \n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.60950255393982\n"
     ]
    }
   ],
   "source": [
    "# Estimated excecution time: 13s\n",
    "init = time.time()\n",
    "\n",
    "saving.saving_metrics(all_metrics_DCSD,all_metrics_DCTIF,all_metrics_VG)\n",
    "saving.saving_sequences(seq_metrics_DCSD,seq_metrics_DCTIF,seq_metrics_VG)\n",
    "saving.saving_global_metrics(global_metrics_DCSD,global_metrics_DCTIF,global_metrics_VG)\n",
    "saving.saving_local_metrics(local_metrics_DCSD,local_metrics_DCTIF,local_metrics_VG)\n",
    "\n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculando as distâncias "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visibility Graphs - VG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "242.17448163032532\n"
     ]
    }
   ],
   "source": [
    "# VG\n",
    "# Tempo de execução: 239.57s\n",
    "init = time.time()\n",
    "\n",
    "vg_metrics_df = pd.read_csv(\"VG-metrics.csv\") # Cada linha do DataFrame é um vetor de características\n",
    "vg_degrees_df = pd.read_csv(\"VG-degrees.csv\") # Cada coluna representa a distribuição de graus de uma rede\n",
    "vg_betweenness_df = pd.read_csv(\"VG-betweenness.csv\") # Cada coluna representa a distribuição de betweenness de uma rede\n",
    "vg_closeness_df = pd.read_csv(\"VG-closeness.csv\") # Cada coluna representa a distribuição de closeness de uma rede\n",
    "\n",
    "vg_metrics_array = vg_metrics_df.to_numpy() # Cada posição do array possui um vetor de características\n",
    "vg_degrees_array = vg_degrees_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de graus\n",
    "vg_betweenness_array = vg_betweenness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de betweenness\n",
    "vg_closeness_array = vg_closeness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de closeness\n",
    "\n",
    "# Calculando as métricas entre todos os vetores de características.\n",
    "\n",
    "dist_vg_metrics = comparing(vg_metrics_array)\n",
    "\n",
    "\n",
    "euclidean_dist_vg_metrics = dist_vg_metrics.euclidean() # Não é capaz de captar correlação\n",
    "manhattan_dist_vg_metrics = dist_vg_metrics.manhattan() # Não é capaz de captar correlação\n",
    "canberra_dist_vg_metrics = dist_vg_metrics.canberra() # Não é capaz de captar correlação\n",
    "cosine_dist_vg_metrics = dist_vg_metrics.cosine() # Não é capaz de captar correlação\n",
    "jensenshannon_dist_vg_metrics = dist_vg_metrics.jensenshannon() # Não é capaz de captar correlação \n",
    "\n",
    "# Calculando as métricas entre todos os vetores com distribuição de graus.\n",
    "\n",
    "dist_vg_degrees = comparing(vg_degrees_array)\n",
    "\n",
    "euclidean_dist_vg_degrees = dist_vg_degrees.euclidean() # Há correlação, mas a definição do limiar é problemática\n",
    "manhattan_dist_vg_degrees = dist_vg_degrees.manhattan() # Limiar consistente. Teste em: <18000\n",
    "canberra_dist_vg_degrees = dist_vg_degrees.canberra() # Limiar pouco consistente. Teste em: <330\n",
    "cosine_dist_vg_degrees = dist_vg_degrees.cosine() # Limiar consistente. Teste em: <0.27\n",
    "jensenshannon_dist_vg_degrees = dist_vg_degrees.jensenshannon() # Limiar consistente. Teste em: <0.27\n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de betweenness.\n",
    "\n",
    "dist_vg_betweenness = comparing(vg_betweenness_array)\n",
    "\n",
    "euclidean_dist_vg_betweenness = dist_vg_betweenness.euclidean() #\n",
    "manhattan_dist_vg_betweenness = dist_vg_betweenness.manhattan() # \n",
    "canberra_dist_vg_betweenness = dist_vg_betweenness.canberra() # \n",
    "cosine_dist_vg_betweenness = dist_vg_betweenness.cosine() # \n",
    "jensenshannon_dist_vg_betweenness = dist_vg_betweenness.jensenshannon() #\n",
    "\n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de closeness.\n",
    "\n",
    "dist_vg_closeness = comparing(vg_closeness_array)\n",
    "\n",
    "euclidean_dist_vg_closeness = dist_vg_closeness.euclidean() # \n",
    "manhattan_dist_vg_closeness = dist_vg_closeness.manhattan() #\n",
    "canberra_dist_vg_closeness = dist_vg_closeness.canberra() #\n",
    "cosine_dist_vg_closeness = dist_vg_closeness.cosine() # \n",
    "jensenshannon_dist_vg_closeness = dist_vg_closeness.jensenshannon() # \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical Characterization using Symbolic Dynamics - DCSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.28383231163025\n"
     ]
    }
   ],
   "source": [
    "# DCSD\n",
    "# Tempo de execução: 177s\n",
    "init = time.time()\n",
    "\n",
    "dcsd_metrics_df = pd.read_csv(\"DCSD-metrics.csv\") # Cada linha do DataFrame é um vetor de características\n",
    "dcsd_degrees_df = pd.read_csv(\"DCSD-degrees.csv\") # Cada coluna representa a distribuição de graus de uma rede\n",
    "dcsd_betweenness_df = pd.read_csv(\"DCSD-betweenness.csv\") # Cada coluna representa a distribuição de betweenness de uma rede\n",
    "dcsd_closeness_df = pd.read_csv(\"DCSD-closeness.csv\") # Cada coluna representa a distribuição de closeness de uma rede\n",
    "\n",
    "dcsd_metrics_array = dcsd_metrics_df.to_numpy() # Cada posição do array possui um vetor de características\n",
    "dcsd_degrees_array = dcsd_degrees_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de graus\n",
    "dcsd_betweenness_array = dcsd_betweenness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de betweenness\n",
    "dcsd_closeness_array = dcsd_closeness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de closeness\n",
    "\n",
    "# Calculando as métricas entre todos os vetores de características.\n",
    "\n",
    "dist_dcsd_metrics = comparing(dcsd_metrics_array)\n",
    "\n",
    "\n",
    "euclidean_dist_dcsd_metrics = dist_dcsd_metrics.euclidean() # \n",
    "manhattan_dist_dcsd_metrics = dist_dcsd_metrics.manhattan() # \n",
    "canberra_dist_dcsd_metrics = dist_dcsd_metrics.canberra() # \n",
    "cosine_dist_dcsd_metrics = dist_dcsd_metrics.cosine() # \n",
    "jensenshannon_dist_dcsd_metrics = dist_dcsd_metrics.jensenshannon() # \n",
    "\n",
    "# Calculando as métricas entre todos os vetores com distribuição de graus.\n",
    "\n",
    "dist_dcsd_degrees = comparing(dcsd_degrees_array)\n",
    "\n",
    "euclidean_dist_dcsd_degrees = dist_dcsd_degrees.euclidean() # \n",
    "manhattan_dist_dcsd_degrees = dist_dcsd_degrees.manhattan() # \n",
    "canberra_dist_dcsd_degrees = dist_dcsd_degrees.canberra() # \n",
    "cosine_dist_dcsd_degrees = dist_dcsd_degrees.cosine() # \n",
    "jensenshannon_dist_dcsd_degrees = dist_dcsd_degrees.jensenshannon() # \n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de betweenness.\n",
    "\n",
    "dist_dcsd_betweenness = comparing(dcsd_betweenness_array)\n",
    "\n",
    "euclidean_dist_dcsd_betweenness = dist_dcsd_betweenness.euclidean() #\n",
    "manhattan_dist_dcsd_betweenness = dist_dcsd_betweenness.manhattan() # \n",
    "canberra_dist_dcsd_betweenness = dist_dcsd_betweenness.canberra() # \n",
    "cosine_dist_dcsd_betweenness = dist_dcsd_betweenness.cosine() # \n",
    "jensenshannon_dist_dcsd_betweenness = dist_dcsd_betweenness.jensenshannon() #\n",
    "\n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de closeness.\n",
    "\n",
    "dist_dcsd_closeness = comparing(dcsd_closeness_array)\n",
    "\n",
    "euclidean_dist_dcsd_closeness = dist_dcsd_closeness.euclidean() # \n",
    "manhattan_dist_dcsd_closeness = dist_dcsd_closeness.manhattan() #\n",
    "canberra_dist_dcsd_closeness = dist_dcsd_closeness.canberra() #\n",
    "cosine_dist_dcsd_closeness = dist_dcsd_closeness.cosine() # \n",
    "jensenshannon_dist_dcsd_closeness = dist_dcsd_closeness.jensenshannon() # \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamical Characterization using the Top Integral Function - DCTIF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "152.7397038936615\n"
     ]
    }
   ],
   "source": [
    "# DCTIF\n",
    "# Tempo de execução: 159s\n",
    "init = time.time()\n",
    "\n",
    "dctif_metrics_df = pd.read_csv(\"DCTIF-metrics.csv\") # Cada linha do DataFrame é um vetor de características\n",
    "dctif_degrees_df = pd.read_csv(\"DCTIF-degrees.csv\") # Cada coluna representa a distribuição de graus de uma rede\n",
    "dctif_betweenness_df = pd.read_csv(\"DCTIF-betweenness.csv\") # Cada coluna representa a distribuição de betweenness de uma rede\n",
    "dctif_closeness_df = pd.read_csv(\"DCTIF-closeness.csv\") # Cada coluna representa a distribuição de closeness de uma rede\n",
    "\n",
    "dctif_metrics_array = dctif_metrics_df.to_numpy() # Cada posição do array possui um vetor de características\n",
    "dctif_degrees_array = dctif_degrees_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de graus\n",
    "dctif_betweenness_array = dctif_betweenness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de betweenness\n",
    "dctif_closeness_array = dctif_closeness_df.T.to_numpy() # Cada posição do array guarda o vetor com a distribuição de closeness\n",
    "\n",
    "# Calculando as métricas entre todos os vetores de características.\n",
    "\n",
    "dist_dctif_metrics = comparing(dctif_metrics_array)\n",
    "\n",
    "\n",
    "euclidean_dist_dctif_metrics = dist_dctif_metrics.euclidean() # \n",
    "manhattan_dist_dctif_metrics = dist_dctif_metrics.manhattan() # \n",
    "canberra_dist_dctif_metrics = dist_dctif_metrics.canberra() # \n",
    "cosine_dist_dctif_metrics = dist_dctif_metrics.cosine() # \n",
    "jensenshannon_dist_dctif_metrics = dist_dctif_metrics.jensenshannon() # \n",
    "\n",
    "# Calculando as métricas entre todos os vetores com distribuição de graus.\n",
    "\n",
    "dist_dctif_degrees = comparing(dctif_degrees_array)\n",
    "\n",
    "euclidean_dist_dctif_degrees = dist_dctif_degrees.euclidean() # \n",
    "manhattan_dist_dctif_degrees = dist_dctif_degrees.manhattan() # \n",
    "canberra_dist_dctif_degrees = dist_dctif_degrees.canberra() # \n",
    "cosine_dist_dctif_degrees = dist_dctif_degrees.cosine() # \n",
    "jensenshannon_dist_dctif_degrees = dist_dctif_degrees.jensenshannon() # \n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de betweenness.\n",
    "\n",
    "dist_dctif_betweenness = comparing(dctif_betweenness_array)\n",
    "\n",
    "euclidean_dist_dctif_betweenness = dist_dctif_betweenness.euclidean() #\n",
    "manhattan_dist_dctif_betweenness = dist_dctif_betweenness.manhattan() # \n",
    "canberra_dist_dctif_betweenness = dist_dctif_betweenness.canberra() # \n",
    "cosine_dist_dctif_betweenness = dist_dctif_betweenness.cosine() # \n",
    "jensenshannon_dist_dctif_betweenness = dist_dctif_betweenness.jensenshannon() #\n",
    "\n",
    "\n",
    "# Calculando as métricas entre todos os vetores com a sequência de closeness.\n",
    "\n",
    "dist_dctif_closeness = comparing(dctif_closeness_array)\n",
    "\n",
    "euclidean_dist_dctif_closeness = dist_dctif_closeness.euclidean() # \n",
    "manhattan_dist_dctif_closeness = dist_dctif_closeness.manhattan() #\n",
    "canberra_dist_dctif_closeness = dist_dctif_closeness.canberra() #\n",
    "cosine_dist_dctif_closeness = dist_dctif_closeness.cosine() # \n",
    "jensenshannon_dist_dctif_closeness = dist_dctif_closeness.jensenshannon() # \n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(end-init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
